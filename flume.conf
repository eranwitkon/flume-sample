# Map agentTest with source and channel
agentTest.sources = sampleDirSource
agentTest.channels = memoryChannel
agentTest.sinks = sampleHDFSSink

#####SOURCE
# Setting the source to spool directory where the file exists
agentTest.sources.sampleDirSource.type = spooldir
agentTest.sources.sampleDirSource.spoolDir = /home/cto/Downloads/flume/spoolSource
# return the full path of the file 
agentTest.sources.sampleDirSource.fileHeader = true
# This is the key used in the sink to reference the full file name
agentTest.sources.sampleDirSource.fileHeaderKey = myFileKey
# return just the file name without the path
agentTest.sources.sampleDirSource.basenameHeader = true
# This is the key used in the sink to reference the file name
agentTest.sources.sampleDirSource.basenameHeaderKey = myBaseKey
# Directory to store metadata related to processing of files. 
agentTest.sources.sampleDirSource.trackerDir = /home/cto/Downloads/flume/.flumespool

#####INTERCEPTORS
# here we chain UUID interceptor and timestemp interceptor
# agentTest.sources.sampleDirSource.interceptors = uuidInterceptor timestampInterceptor
agentTest.sources.sampleDirSource.interceptors = timestampInterceptor
# agentTest.sources.sampleDirSource.interceptors.uuidInterceptor.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
# agentTest.sources.sampleDirSource.interceptors.uuidInterceptor.headerName = id
# agentTest.sources.sampleDirSource.interceptors.uuidInterceptor.preserveExisting = true
# agentTest.sources.sampleDirSource.interceptors.uuidInterceptor.prefix = "flume_"
agentTest.sources.sampleDirSource.interceptors.timestampInterceptor.type = org.apache.flume.interceptor.TimestampInterceptor$Builder


#####CHANNEL
# Setting the channel to memory
agentTest.channels.memoryChannel.type = memory
# Max number of events stored in the memory channel
agentTest.channels.memoryChannel.capacity = 10000
agentTest.channels.memoryChannel.transactioncapacity = 1000



#####SINK
# Setting the sink to HDFS
agentTest.sinks.sampleHDFSSink.type = hdfs
agentTest.sinks.sampleHDFSSink.hdfs.path = hdfs://localhost:8020/data/flumeTest/%{myFileKey}
agentTest.sinks.sampleHDFSSink.hdfs.fileType = DataStream
# This prefix "." in front of temp file making it unreadable to other application
agentTest.sinks.sampleHDFSSink.hdfs.inUsePrefix =.
#hdfs.rollSize: File size to trigger roll, in bytes (0: never roll based on file size)
agentTest.sinks.sampleHDFSSink.hdfs.rollCount = 0
agentTest.sinks.sampleHDFSSink.hdfs.rollInterval = 0
agentTest.sinks.sampleHDFSSink.hdfs.rollSize = 10000000
agentTest.sinks.sampleHDFSSink.hdfs.idleTimeout = 10
# agentTest.sinks.sampleHDFSSink.hdfs.batchSize = 100
# this replace the timestemp interceptor and uses the hadoop local timestemp
#agentTest.sinks.sampleHDFSSink.hdfs.useLocalTimeStamp = true
# Write format can be text or writable
agentTest.sinks.sampleDirSource.hdfs.writeFormat = Text
# use a single file at a time
# agentTest.sinks.sampleDirSource.hdfs.maxOpenFiles = 1


# Connect source and sink with channel
agentTest.sources.sampleDirSource.channels = memoryChannel
agentTest.sinks.sampleHDFSSink.channel = memoryChannel
